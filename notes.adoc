= preparation talk

Bonjour, merci d'être venu assister à cette présentation de l'outil open source Kapoeira.
Non, nous n'allons pas danser, (désolé!) mais nous allons vous parler de comment nous développons et utilisons cet outil
dans notre quotidien pour tester notre pipeline de données qui s'appuie sur des Kafka streams.

Il s'agit d'une conférence de niveau débutant, qui s'adresse aux DEV, aux QA, aux PO, mais la connaissance du film "la cité de la peur"
est un vrai plus pour ce talk.

On va commencer par se présenter
- Mehdi
- Johanna
- François ?

En sortant de ce talk, vous aurez à disposition un nouvel outil pour tester vos kafka streams.
Vous pourrez vous appuyer sur ce nouvel outil pour communiquer entre vous, que vous soyez PO/DEV ou QA.
On vous donnera les clés et astuces pour l'utiliser dans votre quotidien.
Et pour finir, on espère que vous aurez passé un bon moment.

Revenons à présent sur Kapoeira, et sur son histoire.

Pour ça on doit vous parler un peu de notre travail,
Lectra fabrique des machines qui permettent de découper du tissu, pour faire des vêtements, des meubles, des sièges de voiture.
C'est comme des gros ciseaux mais qui ressemblent à ça ! et ça permet d'économiser bcp de tissu et de temps grâce à une
technologie à la pointe notamment en découpant plusieurs piles de tissus à la fois.

Et ces machines, c'est comme si elles avaient toutes un compte Instagram, qu'on appelle la data-collect, et elle poste ce qu'elles
font toute la journée - je suis allumée, je découpe, je fais une pause, je suis  en panne, j'ai découpé 3m de tissu en 2 minutes etc.
Notre travail va consister à collecter tous ces messages et à les combiner à d'autres sources d'informations, par exemple,
le référentiel des machines du client ou les calendriers de rotation des équipes qui travaille sur la machine.
Et on va combiner ces messages et ces infos pour sortir des KPI, comme des vitesses de découpe, des temps d'interruption par jour,
des indices de performance, et on va mettre toutes ces infos dans une base de données en utilisant la plateforme Snowflake.

Et derrière les rouages, on va retrouver la technologie Kafka, et ce qu'on appelle des Kafka streams. Ce sont des petites briques qui
 vont nous permettre de collecter, agréger, transformer la donnée, et chacune de ces briques vont former notre pipeline de données.

Et bien sûr, nous ne faisons jamais d'erreur, n'est-ce pas ?...
Il n'y a jamais de bug dans nos kafka stream non ?
En plus on pense toujours à tous les cas quand on code, c'est bien connu !

C'est faux ! Nous créons des bugs, et en plus si nos données sont erronés, elles se propagent vis tous les autres streams jusqu'en
base de données.
Et là pour corriger, c'est pas drôle, surtout qu'on s'en rend compte souvent trop tard.

C'est quoi du coup la solution Kara ?
Et bien ça va être de tester nos streams !
Ah oui c'est bien les tests. Et pour ça Kafka nous fournit une librairie d'utilitaire, comme les topology test driver.
C'est rapide efficace et pas cher ! Regardez comme c'est beau tout ce scala.
Bon on est content, on peut finir le talk là dessus ?

Ben en fait c'est pas parfait, déjà, ça s'appuie sur des streams factices, et pas sur la vraie infrastructure avec le cluster Kakfa.
Ah.
Et en plus, ça teste un seul stream, unitairement, mais comment on fait si on veut tester plusieurs streams,
via des tests end to end par exemple.
Bonne question
Et puis là c'est bien our les développeurs, mais les QA ils vont pas se mettre à scala pour écrire leurs tests.
C'est pas faux
Ce qui me plairait c'est de pouvoir écrire un test visible par tous, QA, PO DEV qui pourrait nous aider à définir les critères
d'acceptance des stories.
Bon attends là tu en demandes beaucoup, on récapitule :
J'aimerais avoir un outil qui permet à la fois d'écrire des tests d'intégration, des tests end-to-end mais avec une syntax simple
et lisible.
Et mais ça me fait beaucoup penser à Karate ça, tu connais ? C'est un framework qui permet de tester les API HTTP et qui s'appuie
sur la syntaxe Gherkin.
C'est pas mal, on pourrait peut s'en inspirer et le développer pour nos Kafka Streams ?
Et c'est ce qu'on a fait !

Sais-tu danser la Kapoeira, c'est du cucumber et du kafka !
C'est quoi Cucumber ?
...TODO

Vici un exemple avec une calculatrice. Le premier test permet de calculer la somme de 2 variables, et le second la multiplication.
Et voici la GLUE qui se trouve derrière. On va utiliser des regex pour interpreter le test gherkin, et ses assert.

Dans notre cas, la partie cucumber sera codé en scala, il liera la syntaxe Gherlin on a défini et s'occupera de faire des appels
à Kakfa sur un véritable environnement.

Lors d'un test Kapoeira, on va commencer par produire des messages dans un topic d'entrée, on laissera un peu de temps à notre kafka
stream pour faire son travail, puis on va consommer les messages dans le topic de sortie et vérifier qu'on retrouve bien le message
attendu.

C'est ainsi qu'est né Kapoeira en 2020. On a travaillé avec notre QA pour définir un langage simple et explicite.
En première implémentation, on s'appuyait sur des appels back end à Confluent Cli